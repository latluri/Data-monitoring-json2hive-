{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.0.2.6.5.0-292\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.15 (default, May  1 2018 23:32:55)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import commands\n",
    "import ast\n",
    "from StringIO import StringIO\n",
    "import itertools\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "#from pyspark.sql import SparkSession\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "if not os.path.isdir(spark_home):\n",
    "    raise ValueError('SPARK_HOME environment variable is not a directory')\n",
    "if not os.path.isdir(os.path.join(spark_home, 'python')):\n",
    "    raise ValueError('SPARK_HOME directory does not contain python')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "pylib_list = (item for item in os.listdir(os.path.join(spark_home, 'python/lib/'))\n",
    "              if re.match(r'py4j-\\d+(\\.\\d+)+-src\\.zip\\Z', item)\n",
    "              )\n",
    "try:\n",
    "    py4j_file = max(pylib_list)\n",
    "    py4j = os.path.join(spark_home, os.path.join('python/lib', py4j_file))\n",
    "except ValueError:\n",
    "    raise ValueError(\n",
    "        'Could not find py4j'\n",
    "    )\n",
    "sys.path.insert(0, py4j)\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "if os.path.exists(spark_release_file) and \"Spark\" in  open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \" --master yarn-client \\\n",
    "                                         --executor-memory 4g --executor-cores 5 --driver-memory 16g\"\n",
    "                                        )\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "with open(os.path.join(spark_home, 'python/pyspark/shell.py')) as f:\n",
    "    code = compile(f.read(), os.path.join(spark_home, 'python/pyspark/shell.py'), 'exec')\n",
    "    exec(code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert DDLs to csv using DESDDL2CSV.sh and use them as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_today=pd.read_csv(\"/s/latluri1/DES/DES_20190430_processed.csv\",delimiter=\",\").apply(lambda x: x.astype(str).str.upper())\n",
    "df_yesterday=pd.read_csv(\"/s/latluri1/DES/DESscript_20190104_processed.csv\",delimiter=\",\").apply(lambda x: x.astype(str).str.upper())\n",
    "df_yesterday2=df_yesterday.apply(lambda x: x.astype(str).str.lower())\n",
    "df_today2=df_today.apply(lambda x: x.astype(str).str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_list=['account','activityparty','annotation','appointment','businessunit','calendar','calendarrule','contact','cxlvhlp_chatactivity','cxlvhlp_chatqueuestatistic','cxlvhlp_surveyitem','email','fax','gcct_accountresponsibleagent','gcct_additionalsymptomcodes','gcct_addportalmessage','gcct_arbitrationclaimprocessing','gcct_buybackevaluationmilestones','gcct_caseassignment','gcct_caseclassification','gcct_casedispositiontype','gcct_coachback','gcct_country','gcct_customersatisfactiontools','gcct_delegationofauthority','gcct_demandltrtpsmclaimsprocessing','gcct_doaprogramcode','gcct_documentcustomerrecontact','gcct_engine','gcct_executive','gcct_executiveliaison','gcct_fieldinvolvementassistancerequest','gcct_fmcccontract','gcct_fulfillmentvendor','gcct_genericinformation','gcct_genericinformationtopic','gcct_generictopic','gcct_goodfaithreview','gcct_knownpartsdelay','gcct_lemonlawcriteria','gcct_loanercost','gcct_loyaltyassistance','gcct_loyaltycostdetails','gcct_loyaltyprogramcode','gcct_material','gcct_materialrequest','gcct_materialrequestdetail','gcct_offer','gcct_offerdetail','gcct_onlinegaragevehicles','gcct_partsorderstatus','gcct_partsordersystems','gcct_partssmeassistancerequest','gcct_pcarequest','gcct_priordealerdecision','gcct_qaincident','gcct_qamonitoring','gcct_queuepriority','gcct_ravprocessingmilestones','gcct_reactdata','gcct_reacttransmission','gcct_reasoncode','gcct_recall','gcct_region','gcct_rentaldetail','gcct_slaconfiguration','gcct_sms','gcct_socialmediaconversation','gcct_socialmediamessage','gcct_socialmediaprofile','gcct_specialloanercode','gcct_state','gcct_surveyconfiguration','gcct_symptomclassification','gcct_tasktype','gcct_technicalassistancerequest','gcct_testdriverequest','gcct_timezone','gcct_userdelegationofauthority','gcct_vehicle','gcct_vehiclebrand','gcct_vehicleclass','gcct_vehicleloyaltyallowance','gcct_vehicleoffroad','gcct_vehicleowner','gcct_warrantyhistory','gcct_warrantyloanerpartsdelay','gcct_warrantyloanerrequest','gcct_warrantyloanerrequesthistory','globaloptionsetmetadata','incident','incidentresolution','letter','msdyn_answer','msdyn_question','msdyn_questionresponse','msdyn_survey','msdyn_surveyinvite','msdyn_surveyresponse','optionsetmetadata','phonecall','queue','queueitem','sla','slaitem','slakpiinstance','socialactivity','socialprofile','statemetadata','statusmetadata','systemuser','task','team','teammembership','territory']\n",
    "def priority(r):\n",
    "    if(r in tab_list):\n",
    "        return(\"True\")\n",
    "    else:\n",
    "        return(\"False\")\n",
    "    \n",
    "    \n",
    "def get_decimal(a):\n",
    "    if(a['DATA_TYPE']=='decimal'):\n",
    "        if np.logical_not(pd.isnull(a['NUMERIC_PRECISION'])):\n",
    "            if np.logical_not(pd.isnull(a['NUMERIC_SCALE'])):\n",
    "                return(str(a['DATA_TYPE'])+\"(\"+str(int(float(a['NUMERIC_PRECISION'])))+\",\"+str(int(float(a['NUMERIC_SCALE'])))+\")\")\n",
    "            else:\n",
    "                return(str(a['DATA_TYPE'])+\"(\"+str(int(float(a['NUMERIC_PRECISION'])))+\")\")\n",
    "        else:\n",
    "            return(a['DATA_TYPE'])\n",
    "    else:\n",
    "        return(a['DATA_TYPE'])\n",
    "\n",
    "\n",
    "tab_removed_1=set(list(df_yesterday2.TABLE_NAME)).difference(set(list(df_today2.TABLE_NAME)))\n",
    "tab_added_1=set(list(df_today2.TABLE_NAME)).difference(set(list(df_yesterday2.TABLE_NAME)))\n",
    "\n",
    "\n",
    "df_today2.loc[:,'DATA_TYPE']=df_today2[['DATA_TYPE','NUMERIC_PRECISION','NUMERIC_SCALE']].apply(lambda r: get_decimal(r), axis=1)\n",
    "df_yesterday2.loc[:,'DATA_TYPE']=df_yesterday2[['DATA_TYPE','NUMERIC_PRECISION','NUMERIC_SCALE']].apply(lambda r: get_decimal(r), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=df_today2.merge(df_yesterday2,indicator = True, how='outer',on=['TABLE_NAME','COLUMN_NAME'],suffixes=('_new','_old'))\n",
    "final.rename(columns={'_merge':'Change'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"Change\"]=final.Change.replace(\"left_only\",\"Column added\").replace(\"right_only\",\"Column removed\").replace(\"both\",\"No change\")\n",
    "final.loc[(final['DATA_TYPE_old'] != final['DATA_TYPE_new']) & (final.Change ==\"No change\"),\"Change\"]=\"Dtype_change\"\n",
    "#final.loc[(final['Numeric_precision_scale_old'] == final['Numeric_precision_scale_new']) & (final.Change ==\"no change\"),\"Change\"]=\"dtype_change\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.loc[final.TABLE_NAME.isin(tab_removed_1),\"Change\"]=\"Table removed\"\n",
    "final.loc[final.TABLE_NAME.isin(tab_added_1),\"Change\"]=\"Table Added\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=final.loc[final.Change !=\"No change\",[\"TABLE_NAME\",\"COLUMN_NAME\",\"DATA_TYPE_new\",\"DATA_TYPE_old\",\"Change\"]]\n",
    "result[\"priority\"]=result[\"TABLE_NAME\"].apply(lambda r: priority(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(result) >0:\n",
    "    result.to_excel(\"Comp_2_ddls.xlsx\",index=False)\n",
    "    commands.getstatusoutput(\"echo -e \\\"Compared two ddls and observed the output attached\\\"| mailx -s 'Two ddls comparison' -a \\\"Comp_2_ddls.xlsx\\\" latluri1@ford.com  \")\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
