{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import commands\n",
    "import ast\n",
    "import itertools\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "#from pyspark.sql import SparkSession\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "if not os.path.isdir(spark_home):\n",
    "    raise ValueError('SPARK_HOME environment variable is not a directory')\n",
    "if not os.path.isdir(os.path.join(spark_home, 'python')):\n",
    "    raise ValueError('SPARK_HOME directory does not contain python')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "pylib_list = (item for item in os.listdir(os.path.join(spark_home, 'python/lib/'))\n",
    "              if re.match(r'py4j-\\d+(\\.\\d+)+-src\\.zip\\Z', item)\n",
    "              )\n",
    "try:\n",
    "    py4j_file = max(pylib_list)\n",
    "    py4j = os.path.join(spark_home, os.path.join('python/lib', py4j_file))\n",
    "except ValueError:\n",
    "    raise ValueError(\n",
    "        'Could not find py4j'\n",
    "    )\n",
    "sys.path.insert(0, py4j)\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "if os.path.exists(spark_release_file) and \"Spark\" in  open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \" --master yarn-client \\\n",
    "                                         --executor-memory 4g --executor-cores 5 --driver-memory 16g\"\n",
    "                                        )\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "with open(os.path.join(spark_home, 'python/pyspark/shell.py')) as f:\n",
    "    code = compile(f.read(), os.path.join(spark_home, 'python/pyspark/shell.py'), 'exec')\n",
    "    exec(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=spark.read.option(\"header\", \"true\").csv(\"tnedicca_2019_raw/Tnedicca_WA_hotspots_20181204.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"use \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|awk -F'/' '{print $7}'\n",
    "list_=commands.getoutput(\"hdfs dfs -ls /project/dz/collab/DataAcquisition/tnedicca_2018_raw |awk -F' ' '{print $8}'\").splitlines()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.split(\".\")[0]\n",
    "hotspots=[str(i) for i in list_ if \"hot\" in str(str(i).split(\".\")[0]).lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.split(\".\")[0]\n",
    "vehicle=[str(i) for i in list_ if \"vehicle\" in str(i).split(\".\")[0].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_FL_Vehicle_20181204.csv.gz', 'tnedicca_crashdata_vehicle_fl_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_KS_Vehicle_20181220.csv.gz', 'tnedicca_crashdata_vehicle_ks_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_MD_Vehicle_20181204.csv.gz', 'tnedicca_crashdata_vehicle_md_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_ND_vehicle_20181220.csv.gz', 'tnedicca_crashdata_vehicle_nd_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_SD_Vehicle_20181220.csv.gz', 'tnedicca_crashdata_vehicle_sd_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_TX_Vehicle_20181204.csv.gz', 'tnedicca_crashdata_vehicle_tx_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_UT_Vehicle_20181220.csv.gz', 'tnedicca_crashdata_vehicle_ut_2018')\n",
      "('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_VT_vehicle_20181220.csv.gz', 'tnedicca_crashdata_vehicle_vt_2018')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in vehicle:\n",
    "    #col=spark.read.option(\"header\", \"true\").csv(i).columns\n",
    "    j=str(i).split(\"/\")[-1].split(\".\")[0].split(\"_\")[1].lower()\n",
    "    k=\"tnedicca_crashdata_vehicle_\"+j+\"_2018\"\n",
    "    df=spark.read.option(\"header\", \"true\").csv(i)\n",
    "    df.write.format(\"orc\").mode(\"append\").saveAsTable(\"DataAcquisition.\"+k)\n",
    "    print(i,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[report_number: string, crash_date: string, crash_time: string, crash_street: string, crash_city: string, crash_long: string, crash_lat: string, weather_condition: string, light_condition: string, surface_condition: string, crash_type: string, pedestrain_bike_ind: string, dui_alchohol_ind: string, hotspot_id: string, hotspot_size: string, hotspot_centroid_lat: string, hotspot_centroid_long: string, state: string, hotspot_description: string, load_id: string]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "base=spark.read.option(\"header\", \"true\").csv('/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_AZ_hotspots_20181204.csv.gz')\n",
    "for i in hotspots[1:]:\n",
    "    df=spark.read.option(\"header\", \"true\").csv(i)\n",
    "    base=base.unionByName(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dz/collab/DataAcquisition/tnedicca_2018_raw/Tnedicca_GA_hotspots_20181204.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# files where state value is null\n",
    "for i in hotspots:\n",
    "    df=spark.read.option(\"header\", \"true\").csv(i)\n",
    "    if df.where(\"state is null\").count()>0:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|state|count  |\n",
      "+-----+-------+\n",
      "|AZ   |587570 |\n",
      "|SC   |649799 |\n",
      "|MN   |379110 |\n",
      "|NJ   |1070778|\n",
      "|DC   |116515 |\n",
      "|OR   |265762 |\n",
      "|VA   |587280 |\n",
      "|null |7      |\n",
      "|KY   |794849 |\n",
      "|MI   |1511876|\n",
      "|NV   |233736 |\n",
      "|ID   |107173 |\n",
      "|CA   |2108805|\n",
      "|CT   |535702 |\n",
      "|MT   |111122 |\n",
      "|VT   |64888  |\n",
      "|MD   |532404 |\n",
      "|MO   |720788 |\n",
      "|IL   |1473402|\n",
      "|ME   |156759 |\n",
      "|ND   |80983  |\n",
      "|WA   |601283 |\n",
      "|MS   |396261 |\n",
      "|IN   |1057928|\n",
      "|OH   |1448837|\n",
      "|TN   |1155337|\n",
      "|NM   |207407 |\n",
      "|IA   |267544 |\n",
      "|PA   |630330 |\n",
      "|SD   |87648  |\n",
      "|NY   |2258310|\n",
      "|TX   |2908291|\n",
      "|GA   |1999899|\n",
      "|MA   |660687 |\n",
      "|KS   |297814 |\n",
      "|FL   |3282422|\n",
      "|OK   |322853 |\n",
      "|UT   |260725 |\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base.groupby(\"state\").count().show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "base.write.format(\"orc\").mode(\"append\").saveAsTable(\"DataAcquisition.tnedicca_hotspots_2018\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
