{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import commands\n",
    "import ast\n",
    "import itertools\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "#from pyspark.sql import SparkSession\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "if not os.path.isdir(spark_home):\n",
    "    raise ValueError('SPARK_HOME environment variable is not a directory')\n",
    "if not os.path.isdir(os.path.join(spark_home, 'python')):\n",
    "    raise ValueError('SPARK_HOME directory does not contain python')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "pylib_list = (item for item in os.listdir(os.path.join(spark_home, 'python/lib/'))\n",
    "              if re.match(r'py4j-\\d+(\\.\\d+)+-src\\.zip\\Z', item)\n",
    "              )\n",
    "try:\n",
    "    py4j_file = max(pylib_list)\n",
    "    py4j = os.path.join(spark_home, os.path.join('python/lib', py4j_file))\n",
    "except ValueError:\n",
    "    raise ValueError(\n",
    "        'Could not find py4j'\n",
    "    )\n",
    "sys.path.insert(0, py4j)\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "if os.path.exists(spark_release_file) and \"Spark\" in  open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \" --master yarn-client \\\n",
    "                                         --executor-memory 4g --executor-cores 5 --driver-memory 16g\"\n",
    "                                        )\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "with open(os.path.join(spark_home, 'python/pyspark/shell.py')) as f:\n",
    "    code = compile(f.read(), os.path.join(spark_home, 'python/pyspark/shell.py'), 'exec')\n",
    "    exec(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
